{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of DKT:\n",
    "#### Part 1: Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from deepkt import data_util,deepkt,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis  \n",
    "pool = redis.ConnectionPool(host='ubuntu1', port=6379, decode_responses=True)\n",
    "r = redis.Redis(host='ubuntu1', port=6379, decode_responses=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "students = r.keys()  \n",
    "max_feat = 0\n",
    "for student_id in students:\n",
    "    try:\n",
    "        li=r.lrange(student_id, 0, -1)\n",
    "        if len(li) <= 1:\n",
    "            continue\n",
    "        di=[]\n",
    "        for row in li:\n",
    "            s= row.split(',')\n",
    "            feat = int(s[0])+int(s[1])*2\n",
    "            if feat>max_feat:\n",
    "                max_feat = feat\n",
    "            s.append(feat)\n",
    "            di.append(s)\n",
    "        df=pd.DataFrame(di,columns=['correct','skill_id','skill_with_answer'])\n",
    "        df['correct']=df['correct'].astype(dtype=int)\n",
    "        df['skill_id']=df['skill_id'].astype(dtype=int)\n",
    "        data.append(tuple((df['skill_with_answer'].values[:-1],df['skill_id'].values[1:],df['correct'].values[1:])))\n",
    "    except:\n",
    "        print(student_id)\n",
    "        continue\n",
    "# students.remove('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.remove('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seq = pd.Series(data, index=students)\n",
    "nb_users = len(seq)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=lambda: seq,\n",
    "        output_types=(tf.int32, tf.int32, tf.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_depth = max_feat\n",
    "skill_depth = 91+1\n",
    "batch_size = 32\n",
    "MASK_VALUE = -1.\n",
    "verbose = 1  # Verbose = {0,1,2}\n",
    "best_model_weights = \"weights/bestmodel\"  # File to save the model.\n",
    "log_dir = \"logs\"  # Path to save the logs.\n",
    "optimizer = \"adam\"  # Optimizer to use\n",
    "lstm_units = 50  # Number of LSTM units\n",
    "epochs = 3  # Number of epochs to train\n",
    "dropout_rate = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda feat, skill, label: (\n",
    "            tf.one_hot(feat, depth=features_depth),\n",
    "            tf.concat(\n",
    "                values=[\n",
    "                    tf.one_hot(skill, depth=skill_depth),\n",
    "                    tf.expand_dims(label, -1)\n",
    "                ],\n",
    "                axis=-1\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(\n",
    "        batch_size=batch_size,\n",
    "        padding_values=(MASK_VALUE, MASK_VALUE),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = nb_users // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 0.1\n",
    "validation_fraction =0.1\n",
    "train_set, test_set, val_set = data_util.split_dataset(dataset=dataset,\n",
    "                                                           total_size=length,\n",
    "                                                           test_fraction=test_fraction,\n",
    "                                                           val_fraction=validation_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Data Summary =============\n",
      "Total number of students: 18432\n",
      "Training set size: 14929\n",
      "Validation set size: 1658\n",
      "Testing set size: 1843\n",
      "Number of skills: 92\n",
      "Number of features in the input: 181\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "nb_skills =skill_depth\n",
    "nb_features = features_depth\n",
    "set_sz = length * batch_size\n",
    "test_set_sz = (set_sz * test_fraction)\n",
    "val_set_sz = (set_sz - test_set_sz) * validation_fraction\n",
    "train_set_sz = set_sz - test_set_sz - val_set_sz\n",
    "print(\"============= Data Summary =============\")\n",
    "print(\"Total number of students: %d\" % set_sz)\n",
    "print(\"Training set size: %d\" % train_set_sz)\n",
    "print(\"Validation set size: %d\" % val_set_sz)\n",
    "print(\"Testing set size: %d\" % test_set_sz)\n",
    "print(\"Number of skills: %d\" % nb_skills)\n",
    "print(\"Number of features in the input: %d\" % nb_features)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = deepkt.DKTModel(\n",
    "        nb_features=nb_features,\n",
    "        nb_skills=nb_skills,\n",
    "        hidden_units=lstm_units,\n",
    "        dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(),\n",
    "            metrics.AUC(),\n",
    "            metrics.Precision(),\n",
    "            metrics.Recall()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DKTModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 181)]       0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 181)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 50)          46400     \n",
      "_________________________________________________________________\n",
      "outputs (TimeDistributed)    (None, None, 92)          4692      \n",
      "=================================================================\n",
      "Total params: 51,092\n",
      "Trainable params: 51,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(student_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      2/Unknown - 1s 535ms/step - loss: 0.1778 - binary_accuracy: 0.9108 - auc_1: 0.9872 - precision_1: 0.9088 - recall_1: 0.6673WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.513988). Check your callbacks.\n",
      "    465/Unknown - 11s 23ms/step - loss: 0.0826 - binary_accuracy: 0.9799 - auc_1: 0.9911 - precision_1: 0.9119 - recall_1: 0.9892"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c1a8eeca8d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = student_model.fit(dataset=train_set,\n\u001b[0m\u001b[0;32m      3\u001b[0m                                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mJ:\\Myfiles\\20-2\\大数据计算技术\\pycode\\examples\\deepkt\\deepkt.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, epochs, verbose, callbacks, validation_data, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mwhat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mexpects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         return super(DKTModel, self).fit(x=dataset,\n\u001b[0m\u001b[0;32m    133\u001b[0m                                          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[0;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[1;32m--> 862\u001b[1;33m           val_logs = self.evaluate(\n\u001b[0m\u001b[0;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = student_model.fit(dataset=train_set,\n",
    "                                epochs=epochs,\n",
    "                                verbose=verbose,\n",
    "                                validation_data=val_set,\n",
    "                                callbacks=[\n",
    "                                    tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n",
    "                                    tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n",
    "                                                                       save_best_only=True,\n",
    "                                                                       save_weights_only=True),\n",
    "                                    tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 12ms/step - loss: 0.0471 - binary_accuracy: 0.9847 - auc: 0.9966 - precision: 0.9297 - recall: 0.9890\n"
     ]
    }
   ],
   "source": [
    "student_model.load_weights(best_model_weights)\n",
    "result = student_model.evaluate(test_set, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([  3, 129, 101,  65, 129,   3, 101], dtype=int64),\n",
       "  array([64, 50, 32, 64,  1, 50, 32]),\n",
       "  array([1, 1, 1, 1, 1, 1, 1]))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "student_id = 'U_8033986'\n",
    "li=r.lrange(student_id, 0, -1)\n",
    "if len(li) <= 1:\n",
    "    print(\"没有做题记录，无法预测\")\n",
    "di=[]\n",
    "for row in li:\n",
    "    s= row.split(',')\n",
    "    feat = int(s[0])+int(s[1])*2\n",
    "    if feat>max_feat:\n",
    "        max_feat = feat\n",
    "    s.append(feat)\n",
    "    di.append(s)\n",
    "df=pd.DataFrame(di,columns=['correct','skill_id','skill_with_answer'])\n",
    "df['correct']=df['correct'].astype(dtype=int)\n",
    "df['skill_id']=df['skill_id'].astype(dtype=int)\n",
    "data.append(tuple((df['skill_with_answer'].values[:-1],df['skill_id'].values[1:],df['correct'].values[1:])))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = pd.Series(data, index=[student_id])\n",
    "nb_users = len(seq)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "        generator=lambda: seq,\n",
    "        output_types=(tf.int32, tf.int32, tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_depth = max_feat\n",
    "skill_depth = 91+1\n",
    "batch_size = 32\n",
    "MASK_VALUE = -1.\n",
    "verbose = 1  # Verbose = {0,1,2}\n",
    "best_model_weights = \"weights/bestmodel\"  # File to save the model.\n",
    "log_dir = \"logs\"  # Path to save the logs.\n",
    "optimizer = \"adam\"  # Optimizer to use\n",
    "lstm_units = 50  # Number of LSTM units\n",
    "epochs = 3  # Number of epochs to train\n",
    "dropout_rate = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        lambda feat, skill, label: (\n",
    "            tf.one_hot(feat, depth=features_depth),\n",
    "            tf.concat(\n",
    "                values=[\n",
    "                    tf.one_hot(skill, depth=skill_depth),\n",
    "                    tf.expand_dims(label, -1)\n",
    "                ],\n",
    "                axis=-1\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(\n",
    "        batch_size=1,\n",
    "        padding_values=(MASK_VALUE, MASK_VALUE),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-8e1cc0bbe2ee>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-69-8e1cc0bbe2ee>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    student_id =\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 测试单个学生\n",
    "student_id = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "p =student_model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92451835, 0.9864709 , 0.03341067, 0.9616207 , 0.6442864 ,\n",
       "       0.9507751 , 0.51043624, 0.58850646, 0.97118694, 0.5627289 ,\n",
       "       0.7308159 , 0.83594775, 0.43803224, 0.53803194, 0.89481056,\n",
       "       0.8470025 , 0.597956  , 0.9635488 , 0.99976045, 0.84536135,\n",
       "       0.8132259 , 0.8747394 , 0.9861593 , 0.42026725, 0.40344357,\n",
       "       0.86272776, 0.9731734 , 0.45123354, 0.84522223, 0.96729696,\n",
       "       0.92790276, 0.9829868 , 0.9591638 , 0.83736897, 0.8970469 ,\n",
       "       0.44312084, 0.91761553, 0.3324117 , 0.4663628 , 0.40743768,\n",
       "       0.9312024 , 0.30116743, 0.9342462 , 0.49234852, 0.95417786,\n",
       "       0.5668284 , 0.9774703 , 0.84237736, 0.06274936, 0.6192659 ,\n",
       "       0.92499316, 0.2280215 , 0.48547888, 0.08118367, 0.17802924,\n",
       "       0.97667515, 0.4140955 , 0.96509796, 0.9829143 , 0.9188718 ,\n",
       "       0.9383851 , 0.36076826, 0.4011748 , 0.45156395, 0.97441614,\n",
       "       0.76680756, 0.3680261 , 0.88891757, 0.55423933, 0.9976928 ,\n",
       "       0.892238  , 0.48494303, 0.9939953 , 0.8603007 , 0.9367057 ,\n",
       "       0.8584369 , 0.91035235, 0.9268117 , 0.47964722, 0.03660336,\n",
       "       0.5755908 , 0.37653795, 0.61803   , 0.5367161 , 0.9811747 ,\n",
       "       0.37056148, 0.49273837, 0.01480848, 0.942832  , 0.9474409 ,\n",
       "       0.07788485, 0.38753894], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_json(fdir+'/problem_act_train.json',lines=True,encoding='utf-8')\n",
    "train2 = pd.read_json(fdir+'/problem_act_train_2.json',lines=True,encoding='utf-8')\n",
    "train = pd.concat([train1,train2],ignore_index=True).drop_duplicates()\n",
    "problem_info = pd.read_json(fdir+'/problem_info.json',lines=True,encoding='utf-8')\n",
    "train_data = pd.merge(train,problem_info,on='problem_id')\n",
    "train_data[\"skill\"]=train_data[\"concept\"]\n",
    "for i in range(len(train_data)):\n",
    "    train_data[\"skill\"].iloc[i] = train_data[\"concept\"].iloc[i][0]\n",
    "train_data[\"skill_id\"]=train_data[\"skill\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OrdinalEncoder\n",
    "encoder = OrdinalEncoder(cols = ['skill_id','student_id','problem_id'], \n",
    "                         handle_unknown = 'value', \n",
    "                         handle_missing = 'value').fit(train_data) \n",
    "encoded_train = encoder.transform(train_data)\n",
    "encoded_train['user_id'] =encoded_train['student_id']\n",
    "encoded_train['correct'] =encoded_train['label']\n",
    "encoded_train=encoded_train.drop(['problem_id','student_id','label','concept','detail'],axis=1)\n",
    "encoded_train.to_csv('train_data.csv',index=0,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = \"data/ASSISTments_skill_builder_data.csv\" # Dataset path\n",
    "fn = \"train_data.csv\"\n",
    "verbose = 1 # Verbose = {0,1,2}\n",
    "best_model_weights = \"weights/bestmodel\" # File to save the model.\n",
    "log_dir = \"logs\" # Path to save the logs.\n",
    "optimizer = \"adam\" # Optimizer to use\n",
    "lstm_units = 50 # Number of LSTM units\n",
    "batch_size = 16 # Batch size\n",
    "epochs = 3 # Number of epochs to train\n",
    "dropout_rate = 0.3 # Dropout rate\n",
    "test_fraction = 0.2 # Portion of data to be used for testing\n",
    "validation_fraction = 0.2 # Portion of training data to be used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Data Summary =============\n",
      "Total number of students: 13408\n",
      "Training set size: 8581\n",
      "Validation set size: 2145\n",
      "Testing set size: 2681\n",
      "Number of skills: 61\n",
      "Number of features in the input: 122\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "from deepkt import deepkt, data_util, metrics\n",
    "\n",
    "\n",
    "dataset, length, nb_features, nb_skills = data_util.load_dataset(fn=fn,\n",
    "                                                                 batch_size=batch_size,\n",
    "                                                                 shuffle=True)\n",
    "\n",
    "train_set, test_set, val_set = data_util.split_dataset(dataset=dataset,\n",
    "                                                       total_size=length,\n",
    "                                                       test_fraction=test_fraction,\n",
    "                                                       val_fraction=validation_fraction)\n",
    "\n",
    "\n",
    "set_sz = length * batch_size\n",
    "test_set_sz = (set_sz * test_fraction)\n",
    "val_set_sz = (set_sz - test_set_sz) * validation_fraction\n",
    "train_set_sz = set_sz - test_set_sz - val_set_sz\n",
    "print(\"============= Data Summary =============\")\n",
    "print(\"Total number of students: %d\" % set_sz)\n",
    "print(\"Training set size: %d\" % train_set_sz)\n",
    "print(\"Validation set size: %d\" % val_set_sz)\n",
    "print(\"Testing set size: %d\" % test_set_sz)\n",
    "print(\"Number of skills: %d\" % nb_skills)\n",
    "print(\"Number of features in the input: %d\" % nb_features)\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Part 3: Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DKTModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, None, 122)]       0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 122)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 50)          34600     \n",
      "_________________________________________________________________\n",
      "outputs (TimeDistributed)    (None, None, 61)          3111      \n",
      "=================================================================\n",
      "Total params: 37,711\n",
      "Trainable params: 37,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "student_model = deepkt.DKTModel(\n",
    "                        nb_features=nb_features,\n",
    "                        nb_skills=nb_skills,\n",
    "                        hidden_units=lstm_units,\n",
    "                        dropout_rate=dropout_rate)\n",
    "\n",
    "student_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(),\n",
    "            metrics.AUC(),\n",
    "            metrics.Precision(),\n",
    "            metrics.Recall()\n",
    "        ])\n",
    "\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "history = student_model.fit(dataset=train_set,\n",
    "                            epochs=epochs,\n",
    "                            verbose=verbose,\n",
    "                            validation_data=val_set,\n",
    "                            callbacks=[ \n",
    "                                tf.keras.callbacks.CSVLogger(f\"{log_dir}/train.log\"),\n",
    "                                tf.keras.callbacks.ModelCheckpoint(best_model_weights,\n",
    "                                                                   save_best_only=True,\n",
    "                                                                   save_weights_only=True),\n",
    "                                tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5: Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.load_weights(best_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 1s 6ms/step - loss: 0.0901 - binary_accuracy: 0.9743 - auc_1: 0.9909 - precision_1: 0.9088 - recall_1: 0.9991\n"
     ]
    }
   ],
   "source": [
    "student_model = deepkt.DKTModel(\n",
    "                        nb_features=nb_features,\n",
    "                        nb_skills=nb_skills,\n",
    "                        hidden_units=lstm_units,\n",
    "                        dropout_rate=dropout_rate)\n",
    "\n",
    "student_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(),\n",
    "            metrics.AUC(),\n",
    "            metrics.Precision(),\n",
    "            metrics.Recall()\n",
    "        ])\n",
    "student_model.load_weights('./weights/bestmodel')\n",
    "result = student_model.evaluate(test_set, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": " tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
